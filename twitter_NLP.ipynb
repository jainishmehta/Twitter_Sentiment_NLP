{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def explore_data(df_train, df_test):\n",
    "\n",
    "    # Display basic information about train and test dataframes\n",
    "    print(\"Train Data:\")\n",
    "    print(df_train.head())\n",
    "\n",
    "    print(\"\\nTest Data:\")\n",
    "    print(df_test.head())\n",
    "\n",
    "    # Drop rows with missing 'text' values in the train set\n",
    "    df_train.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "    # Count the occurrences of each target value\n",
    "    target_value_counts = df_train['target'].value_counts()\n",
    "    print(\"Count of Target 1:\", target_value_counts[1])\n",
    "    print(\"Count of Target 0:\", target_value_counts[0])\n",
    "\n",
    "    # Create a new column 'count_words' to store the word count of each text\n",
    "    df_train['count_words'] = df_train['text'].str.split().str.len()\n",
    "\n",
    "    sns.histplot(df_train['count_words'], bins=range(1, 35, 2))\n",
    "\n",
    "# Adjust the file paths accordingly\n",
    "train_file_path = '../input/speechify/data/train.csv'\n",
    "test_file_path = '../input/speechify/data/test.csv'\n",
    "df = pd.read_csv(train_file_path)\n",
    "df_test = pd.read_csv(test_file_path)\n",
    "explore_data(df, df_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess_and_split_data(df, df_test, test_size=0.2, random_state=42):\n",
    "    # Extracting features and target\n",
    "    X = df['text']\n",
    "    y = df['target']\n",
    "    X_test = df_test['text']\n",
    "\n",
    "    # Splitting the data into training and validation sets\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    return X_train, X_valid, y_train, y_valid, X_test\n",
    "\n",
    "# Assuming df and df_test are defined\n",
    "X_train, X_valid, y_train, y_valid, X_test = preprocess_and_split_data(df, df_test)\n",
    "print(X_train)\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider LSTM to account for vanishing gradient problem and since it is a NLP problem\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model(MAX_WORD_LENGTH):\n",
    "    model = Sequential()\n",
    "    model.add(keras.Input(shape=(MAX_WORD_LENGTH,)))\n",
    "    model.add(keras.layers.Embedding(len(tokenizer.word_index), 64))\n",
    "    model.add(keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(), optimizer=Adam(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORD_LENGTH = 35\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def tokenize_and_pad_data(X_train, X_test, max_word_length):\n",
    "    tokenizer = Tokenizer(num_words=max_word_length)\n",
    "    tokenizer.fit_on_texts(X_train) \n",
    "    sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "    X_train_padded = pad_sequences(sequences_train, maxlen=max_word_length) \n",
    "\n",
    "    sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "    X_test_padded = pad_sequences(sequences_test, maxlen=max_word_length)\n",
    "\n",
    "    return X_train_padded, X_test_padded, tokenizer\n",
    "\n",
    "# Assuming X_train, X_test, and y_train are defined elsewhere\n",
    "X_train_padded, X_test_padded, tokenizer = tokenize_and_pad_data(X_train, X_test, MAX_WORD_LENGTH)\n",
    "\n",
    "# Build and train the model\n",
    "model = build_model(MAX_WORD_LENGTH)\n",
    "history = model.fit(X_train_padded, y_train, batch_size=32, epochs=15, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.read_csv('../input/speechify/data/sample_submission.csv')\n",
    "print(df_sample)\n",
    "\n",
    "df_sample['target'] = np.where(model.predict(X_test_padded)>=0.5, 1, 0)\n",
    "print(df_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "tokenizer = Tokenizer(num_words=MAX_WORD_LENGTH)\n",
    "tokenizer.fit_on_texts(X_valid) \n",
    "valid_sequences = tokenizer.texts_to_sequences(X_valid)\n",
    "X_valid_padded = pad_sequences(valid_sequences, maxlen=MAX_WORD_LENGTH) \n",
    "y_valid_estimation = np.where(model.predict(X_valid_padded)>=0.5, 1, 0)\n",
    "print(classification_report(y_valid, y_valid_estimation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_with_stratified_kfold(X, y, build_model_func, MAX_WORD_LENGTH, n_splits=10, epochs=40):\n",
    "    f1_score_kfolds = []\n",
    "    y_test_results = []\n",
    "    \n",
    "    skfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    for train_ix, valid_ix in skfold.split(X, y):\n",
    "        X_train, X_valid = X[train_ix], X[valid_ix]\n",
    "        y_train, y_valid = y[train_ix], y[valid_ix]\n",
    "\n",
    "        tokenizer = Tokenizer(num_words=MAX_WORD_LENGTH)\n",
    "        tokenizer.fit_on_texts(X_train) \n",
    "        sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "        X_train_padded = pad_sequences(sequences_train, maxlen=MAX_WORD_LENGTH) \n",
    "\n",
    "        sequences_valid = tokenizer.texts_to_sequences(X_valid)\n",
    "        X_valid_padded = pad_sequences(sequences_valid, maxlen=MAX_WORD_LENGTH) \n",
    "\n",
    "        sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "        X_test_padded = pad_sequences(sequences_test, maxlen=MAX_WORD_LENGTH) \n",
    "\n",
    "        model = build_model_func(MAX_WORD_LENGTH)\n",
    "        history = model.fit(X_train_padded, y_train, batch_size=32, epochs=epochs, verbose=2, validation_split=0.2)\n",
    "        \n",
    "        y_valid_estimation = np.where(model.predict(X_valid_padded) >= 0.5, 1, 0)\n",
    "        f1 = f1_score(y_valid, y_valid_estimation)\n",
    "        print(\"F1 Score is\", f1)\n",
    "        f1_score_kfolds.append(f1)\n",
    "        \n",
    "        y_test_results.append(model.predict(X_test_padded))\n",
    "\n",
    "    return f1_score_kfolds, y_test_results\n",
    "\n",
    "X = df['text']\n",
    "y = df['target']\n",
    "f1_scores, y_test_results = train_with_stratified_kfold(X, y, build_model, MAX_WORD_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_scores)\n",
    "df_sample = pd.read_csv('../input/speechify/data/sample_submission.csv')\n",
    "\n",
    "df_sample['target'] = np.where(np.mean(y_test_results, axis=0)>=0.5, 1, 0)\n",
    "print(df_sample)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
